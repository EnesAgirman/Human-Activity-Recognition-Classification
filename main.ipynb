{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['trX', 'trY', 'tstX', 'tstY']>\n"
     ]
    }
   ],
   "source": [
    "# Load and open the file containing the data\n",
    "myFile = h5py.File('data-Mini Project 2.h5', 'r+')\n",
    "\n",
    "# List all groups in the .h5 file\n",
    "print(f\"Keys: {myFile.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"trX\": shape (3000, 150, 3), type \"<f8\">\n",
      "<HDF5 dataset \"trY\": shape (3000, 6), type \"<f8\">\n",
      "<HDF5 dataset \"tstX\": shape (600, 150, 3), type \"<f8\">\n",
      "<HDF5 dataset \"tstY\": shape (600, 6), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "# print the information about the keys in the data\n",
    "for key in myFile.keys():\n",
    "    print(myFile[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trX shape: (3000, 150, 3), dtype: float64\n",
      "trY shape: (3000, 6), dtype: float64\n",
      "tstX shape: (600, 150, 3), dtype: float64\n",
      "tstY shape: (600, 6), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Extract the data from the file as numpy arrays\n",
    "\n",
    "n1 = myFile.get('trX')  # trX is the training data\n",
    "trX = np.array(n1)\n",
    "print(f\"trX shape: {trX.shape}, dtype: {trX.dtype}\")\n",
    "\n",
    "n1 = myFile.get('trY')  # trY is the training labels\n",
    "trY = np.array(n1)\n",
    "print(f\"trY shape: {trY.shape}, dtype: {trY.dtype}\")\n",
    "\n",
    "n1 = myFile.get('tstX') # tstX is the test data\n",
    "tstX = np.array(n1)\n",
    "print(f\"tstX shape: {tstX.shape}, dtype: {tstX.dtype}\")\n",
    "\n",
    "n1 = myFile.get('tstY') # tstY is the test labels\n",
    "tstY = np.array(n1)\n",
    "print(f\"tstY shape: {tstY.shape}, dtype: {tstY.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the .h5 file\n",
    "myFile.close()\n",
    "\n",
    "# shuffle the training and the test data\n",
    "indexes = np.arange(trX.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "trX = trX[indexes]\n",
    "trY = trY[indexes]\n",
    "\n",
    "indexes = np.arange(tstX.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "tstX = tstX[indexes]\n",
    "tstY = tstY[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh activation function for the hidden layer\n",
    "def tanh_activation(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "# sigmoid activation function for the output layer\n",
    "def sigmoid_activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# sigmoid derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, trX, trY, tstX, tstY, N, learning_rate, mini_batch_size, num_epochs):\n",
    "        # initialize the data\n",
    "        self.trX = trX  # 3000 x 150 x 3\n",
    "        self.trY = trY  # 3000 x 6\n",
    "        self.tstX = tstX    # 600 x 150 x 3\n",
    "        self.tstY = tstY    # 600 x 6\n",
    "        \n",
    "        # add the bias to the data\n",
    "        self.trX = np.concatenate((self.trX, np.ones((self.trX.shape[0], self.trX.shape[1], 1))), axis=2)   # 3000 x 150 x 4\n",
    "        self.tstX = np.concatenate((self.tstX, np.ones((self.tstX.shape[0], self.tstX.shape[1], 1))), axis=2)   # 600 x 150 x 4\n",
    "\n",
    "        # initialize the hyperparameters\n",
    "        self.N = N\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # initialize the weights and biases\n",
    "        self.Whh = np.random.uniform(-0.1, 0.1, (self.N, self.N+1))   # N x N\n",
    "        self.W1h = np.random.uniform(-0.1, 0.1, (self.N, 3+1))  # N x 4\n",
    "        self.Who = np.random.uniform(-0.1, 0.1, (6, self.N+1))  # 6 x (N+1)\n",
    "\n",
    "    # forward pass\n",
    "    def forward_pass(self, x):\n",
    "        # x is a 150x4 vector where the first 3 elements are the sensor data and the last element is the bias with 150 time steps\n",
    "        # initialize hidden layer\n",
    "        h = np.zeros((self.N, 1))   # N x 1\n",
    "        # initialize hidden layer output\n",
    "        h_out = np.zeros((self.N, 1))   # N x 1\n",
    "        # initialize output layer\n",
    "        y = np.zeros((6, 1))\n",
    "        # initialize hidden states\n",
    "        hidden_states = []\n",
    "        hidden_states.append(np.zeros((self.N, 1))) # initial state\n",
    "        \n",
    "        # initialize the predictions\n",
    "        preds = []\n",
    "\n",
    "        # loop over the time steps\n",
    "        for t in range(x.shape[0]):\n",
    "            # update the hidden layer\n",
    "            h = np.matmul(self.Whh, h) + np.matmul(self.W1h, x[t].reshape(-1, 1))\n",
    "            # update the hidden layer output\n",
    "            h_out = tanh_activation(h)\n",
    "            hidden_states.append(h_out)\n",
    "            # add the bias to the hidden layer output\n",
    "            h_out = np.concatenate((h_out, np.ones((1, 1))), axis=0)\n",
    "            # update the output layer\n",
    "            y = np.matmul(self.Who, h_out)\n",
    "            # apply the sigmoid activation function to the output layer\n",
    "            y = sigmoid_activation(y)\n",
    "            preds.append(y)\n",
    "\n",
    "        return hidden_states, preds\n",
    "    \n",
    "    # multi category cross entropy loss function\n",
    "    def loss_function(self, y, pred):\n",
    "        loss = np.sum(y * np.log(pred) + (1 - y) * np.log(1 - pred))\n",
    "        return -loss\n",
    "\n",
    "    # backpropagation\n",
    "    def backpropagation(self, batch_size):\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i in range(0, self.trX.shape[0], batch_size):\n",
    "                x_batch = np.array_split(self.trX, batch_size)[i]\n",
    "                y_batch = np.array_split(self.trY, batch_size)[i]\n",
    "                \n",
    "                # initialize the gradients\n",
    "                dW1h = np.zeros_like(self.W1h)\n",
    "                dWhh = np.zeros_like(self.Whh)\n",
    "                dWho = np.zeros_like(self.Who)\n",
    "                \n",
    "                dH = np.zeros((self.N, 1)) # initialize the gradient of the hidden layer\n",
    "                \n",
    "                previous_state = np.zeros((self.N, 1)) # initialize the previous state as a zero vector (h_-1 = 0)\n",
    "                \n",
    "                for t in range(self.trX.shape[1]):\n",
    "                    print(x_batch[t].shape)\n",
    "                    hidden_states, preds = self.forward_pass(x_batch[t])\n",
    "                    previous_state = hidden_states[-1]\n",
    "                    pred = preds[-1]\n",
    "                    d_o = ( pred - y_batch[t].reshape(-1, 1) ) * sigmoid_derivative(pred)\n",
    "                    dWho += np.matmul(d_o[:-1], previous_state.T)                  \n",
    "                    dH = np.matmul(self.Who.T, d_o) * tanh_derivative(np.concatenate((previous_state, np.ones((1, 150))), axis=0))\n",
    "                    dWhh += np.matmul(dH, np.concatenate((previous_state, np.ones((1, 150))), axis=0).T)\n",
    "                    dW1h += np.matmul(dH, x_batch[t])\n",
    "                    \n",
    "                    if t == self.trX.shape[1] - 1:\n",
    "                        correctPred = np.sum(np.argmax(pred, axis=0) == np.argmax(y_batch[t].reshape(-1, 1), axis=0))\n",
    "                \n",
    "                # update the weights and biases\n",
    "                self.Whh -= self.learning_rate * dWhh / self.trX.shape[1]\n",
    "                self.W1h -= self.learning_rate * dW1h / self.trX.shape[1]\n",
    "                self.Who -= self.learning_rate * dWho / self.trX.shape[1]\n",
    "                \n",
    "            accuracy = correctPred / trX.shape[0]\n",
    "            print(f\"Epoch: {epoch}, Accuracy: {accuracy}\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "N = 50\n",
    "learning_rate = 0.05\n",
    "mini_batch_size = 30\n",
    "num_epochs = 50\n",
    "rnn = RNN(trX, trY, tstX, tstY, N, learning_rate, mini_batch_size, num_epochs)\n",
    "y, p = rnn.forward_pass(trX[0])\n",
    "print(y[0].shape)\n",
    "print(p[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
