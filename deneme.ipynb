{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['trX', 'trY', 'tstX', 'tstY']>\n"
     ]
    }
   ],
   "source": [
    "# Load and open the file containing the data\n",
    "myFile = h5py.File('data-Mini Project 2.h5', 'r+')\n",
    "\n",
    "# List all groups in the .h5 file\n",
    "print(f\"Keys: {myFile.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"trX\": shape (3000, 150, 3), type \"<f8\">\n",
      "<HDF5 dataset \"trY\": shape (3000, 6), type \"<f8\">\n",
      "<HDF5 dataset \"tstX\": shape (600, 150, 3), type \"<f8\">\n",
      "<HDF5 dataset \"tstY\": shape (600, 6), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "# print the information about the keys in the data\n",
    "for key in myFile.keys():\n",
    "    print(myFile[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trX shape: (3000, 150, 3), dtype: float64\n",
      "trY shape: (3000, 6), dtype: float64\n",
      "tstX shape: (600, 150, 3), dtype: float64\n",
      "tstY shape: (600, 6), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Extract the data from the file as numpy arrays\n",
    "\n",
    "n1 = myFile.get('trX')  # trX is the training data\n",
    "trX = np.array(n1)\n",
    "print(f\"trX shape: {trX.shape}, dtype: {trX.dtype}\")\n",
    "\n",
    "n1 = myFile.get('trY')  # trY is the training labels\n",
    "trY = np.array(n1)\n",
    "print(f\"trY shape: {trY.shape}, dtype: {trY.dtype}\")\n",
    "\n",
    "n1 = myFile.get('tstX') # tstX is the test data\n",
    "tstX = np.array(n1)\n",
    "print(f\"tstX shape: {tstX.shape}, dtype: {tstX.dtype}\")\n",
    "\n",
    "n1 = myFile.get('tstY') # tstY is the test labels\n",
    "tstY = np.array(n1)\n",
    "print(f\"tstY shape: {tstY.shape}, dtype: {tstY.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the average of each example for each sensor over 150 time steps as the 151st time step\n",
    "\n",
    "# trX = np.concatenate((trX, np.mean(trX, axis=1, keepdims=True)), axis=1)\n",
    "# print(f\"trX shape: {trX.shape}, dtype: {trX.dtype}\")\n",
    "\n",
    "# tstX = np.concatenate((tstX, np.mean(tstX, axis=1, keepdims=True)), axis=1)\n",
    "# print(f\"tstX shape: {tstX.shape}, dtype: {tstX.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer activation function\n",
    "def tanh_activation(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# sigmoid activation function\n",
    "def sigmoid_activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, trX, trY, tstX, tstY, N, learning_rate, mini_batch_size, num_epochs):\n",
    "        # initialize the data\n",
    "        self.trX = trX  # 3000 x 150 x 3\n",
    "        self.trY = trY  # 3000 x 6\n",
    "        self.tstX = tstX    # 600 x 150 x 3\n",
    "        self.tstY = tstY    # 600 x 6\n",
    "        \n",
    "        # add the bias to the data\n",
    "        self.trX = np.concatenate((self.trX, np.ones((self.trX.shape[0], self.trX.shape[1], 1))), axis=2)   # 3000 x 150 x 4\n",
    "        self.tstX = np.concatenate((self.tstX, np.ones((self.tstX.shape[0], self.tstX.shape[1], 1))), axis=2)   # 600 x 150 x 4\n",
    "\n",
    "        # initialize the hyperparameters\n",
    "        self.N = N\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # initialize the weights and biases\n",
    "        self.Whh = np.random.uniform(-0.1, 0.1, (self.N, self.N))   # N x N\n",
    "        self.W1h = np.random.uniform(-0.1, 0.1, (self.N, 3+1))  # N x 4\n",
    "        self.Who = np.random.uniform(-0.1, 0.1, (6, self.N+1))  # 6 x (N+1)\n",
    "\n",
    "    # forward pass\n",
    "    def forward_pass(self, x):\n",
    "        # x is a 150x4 vector where the first 3 elements are the sensor data and the last element is the bias with 150 time steps\n",
    "        # initialize hidden layer\n",
    "        h = np.zeros((self.N, 1))   # N x 1\n",
    "        # initialize hidden layer output\n",
    "        h_out = np.zeros((self.N, 1))   # N x 1\n",
    "        # initialize output layer\n",
    "        y = np.zeros((6, 1))\n",
    "\n",
    "        # loop over the time steps\n",
    "        for t in range(x.shape[0]):\n",
    "            # update the hidden layer\n",
    "            ara = np.matmul(self.W1h, x[t].reshape(-1, 1))\n",
    "            h = np.matmul(self.Whh, h) + ara\n",
    "            # update the hidden layer output\n",
    "            h_out = tanh_activation(h)\n",
    "            # add the bias to the hidden layer output\n",
    "            h_out = np.concatenate((h_out, np.ones((1, 1))), axis=0)\n",
    "            # update the output layer\n",
    "            y = np.matmul(self.Who, h_out)\n",
    "            # apply the sigmoid activation function to the output layer\n",
    "            y = sigmoid_activation(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def cross_entropy_loss(self, y, pred):\n",
    "        # calculate the cross entropy loss\n",
    "        loss = -np.sum(np.multiply(y, np.log(pred)) + np.multiply((1 - y), np.log(1 - pred)))\n",
    "        return loss\n",
    "\n",
    "    # backpropagation\n",
    "    def backpropagation(self, x, y, pred):\n",
    "        # initialize the gradients\n",
    "        dW1h = np.zeros_like(self.W1h)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWho = np.zeros_like(self.Who)\n",
    "        \n",
    "        # initialize the hidden layer\n",
    "        h = np.zeros((self.N, 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for t in reversed(range(x.shape[0])):\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "    def backward_pass(self, x, y, pred):\n",
    "        # initialize the error in the output layer\n",
    "        delta_o = np.zeros((6, 1))\n",
    "        # initialize the error in the hidden layer\n",
    "        delta_h = np.zeros((self.N, 1))\n",
    "\n",
    "        # loop over the time steps\n",
    "        for t in reversed(range(x.shape[1])):\n",
    "            # calculate the error in the output layer\n",
    "            delta_o = (pred[:, t].reshape(-1, 1) - y[:, t].reshape(-1, 1)) * pred[:, t].reshape(-1, 1) * (1 - pred[:, t].reshape(-1, 1))\n",
    "            # calculate the error in the hidden layer\n",
    "            delta_h = np.dot(self.Who.T) * delta_o + np.dot(self.Whh.T, delta_h)\n",
    "\n",
    "# Backpropagation Through Time (BPTT) function\n",
    "def bptt(trX, trY, W1h, Whh, Who, learning_rate, N):\n",
    "    hidden_states, outputs = forward_pass(trX, W1h, Whh, Who, N)\n",
    "    training_loss = cross_entropy_loss(outputs, trY)  # Compute training loss\n",
    "    dL_doutputs = outputs - trY\n",
    "\n",
    "    dW1h = np.zeros_like(W1h)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWho = np.zeros_like(Who)\n",
    "\n",
    "    for t in reversed(range(trX.shape[1])):\n",
    "        dWho += np.dot(dL_doutputs.T, hidden_states)\n",
    "        dhidden = np.dot(dL_doutputs, Who) * dtanh(hidden_states)  # dtanh for derivative of tanh\n",
    "\n",
    "        for bptt_step in reversed(range(max(0, t - backprop_truncate), t+1)):\n",
    "            dWhh += np.dot(dhidden.T, hidden_states)\n",
    "            Xt_bias = np.hstack((trX[:, bptt_step, :], np.ones((trX.shape[0], 1))))\n",
    "            dW1h += np.dot(dhidden.T, Xt_bias)\n",
    "            dhidden = np.dot(dhidden, Whh) * dtanh(hidden_states)  # dtanh for derivative of tanh\n",
    "\n",
    "    W1h -= learning_rate * dW1h\n",
    "    Whh -= learning_rate * dWhh\n",
    "    Who -= learning_rate * dWho\n",
    "\n",
    "    return W1h, Whh, Who, training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: [[0.50517189]\n",
      " [0.48755945]\n",
      " [0.51946932]\n",
      " [0.50987327]\n",
      " [0.52375783]\n",
      " [0.46712384]], shape: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "# initialize the network\n",
    "N = 50\n",
    "learning_rate = 0.05\n",
    "mini_batch_size = 30\n",
    "num_epochs = 50\n",
    "rnn = RNN(trX, trY, tstX, tstY, N, learning_rate, mini_batch_size, num_epochs)\n",
    "y = rnn.forward_pass(rnn.trX[453])\n",
    "print(f\"y: {y}, shape: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (50,4) and (150,1) not aligned: 4 (dim 1) != 150 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\NSagi\\Music\\Desktop\\2023-24_Fall\\EEE-443\\Mini-Project_2\\Human-Activity-Recognition-Classification\\deneme.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# loop over the training examples\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(trX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39;49mforward_pass(trX[i, :, :])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# backward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     rnn\u001b[39m.\u001b[39mbackward_pass(trX[i, :, :], trY[i, :, :], y_hat)\n",
      "\u001b[1;32mc:\\Users\\NSagi\\Music\\Desktop\\2023-24_Fall\\EEE-443\\Mini-Project_2\\Human-Activity-Recognition-Classification\\deneme.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# loop over the time steps\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# update the hidden layer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     h \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWhh, h) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW1h, x[:, t]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# update the hidden layer output\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NSagi/Music/Desktop/2023-24_Fall/EEE-443/Mini-Project_2/Human-Activity-Recognition-Classification/deneme.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     h_out \u001b[39m=\u001b[39m tanh_activation(h)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (50,4) and (150,1) not aligned: 4 (dim 1) != 150 (dim 0)"
     ]
    }
   ],
   "source": [
    "# implement the training loop and test the network on the test data\n",
    "# initialize the training loss\n",
    "tr_loss = np.zeros((num_epochs, 1))\n",
    "# initialize the test loss\n",
    "tst_loss = np.zeros((num_epochs, 1))\n",
    "\n",
    "# initialize the network\n",
    "N = 50\n",
    "learning_rate = 0.05\n",
    "mini_batch_size = 30\n",
    "num_epochs = 50\n",
    "rnn = RNN(trX, trY, tstX, tstY, N, learning_rate, mini_batch_size, num_epochs)\n",
    "\n",
    "# loop over the epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize the training loss for this epoch\n",
    "    tr_loss_epoch = 0\n",
    "    # initialize the test loss for this epoch\n",
    "    tst_loss_epoch = 0\n",
    "\n",
    "    # loop over the training examples\n",
    "    for i in range(trX.shape[0]):\n",
    "        # forward pass\n",
    "        y_hat = rnn.forward_pass(trX[i, :, :])\n",
    "        # backward pass\n",
    "        rnn.backward_pass(trX[i, :, :], trY[i, :, :], y_hat)\n",
    "        # update the weights and biases\n",
    "        Who = Who - learning_rate * rnn.delta_o * rnn.h.T\n",
    "        W1h = W1h - learning_rate * rnn.delta_h * rnn.h_out.T\n",
    "        Whh = Whh - learning_rate * rnn.delta_h * rnn.h.T\n",
    "\n",
    "        # calculate the training loss for this example\n",
    "        tr_loss_epoch = tr_loss_epoch + np.sum(np.square(trY[i, :, :] - y_hat))\n",
    "        # calculate the test loss for this example\n",
    "        tst_loss_epoch = tst_loss_epoch + np.sum(np.square(tstY[i, :, :] - y_hat))\n",
    "\n",
    "    # calculate the average training loss for this epoch\n",
    "    tr_loss[epoch] = tr_loss_epoch / trX.shape[0]\n",
    "    # calculate the average test loss for this epoch\n",
    "    tst_loss[epoch] = tst_loss_epoch / tstX.shape[0]\n",
    "\n",
    "    # print the training and test loss for this epoch\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {tr_loss[epoch]}, Test Loss: {tst_loss[epoch]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
